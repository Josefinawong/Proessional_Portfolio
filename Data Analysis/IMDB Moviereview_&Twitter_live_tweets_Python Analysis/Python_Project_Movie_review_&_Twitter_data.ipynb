{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Large Movie Review Dataset"
      ],
      "metadata": {
        "id": "iu0A5GzWYSKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Large Movie Review Dataset is a corpus of 50,000 movie reviews from IMDB that have been classified as either positive or negative. More information about the dataset can be found at https://ai.stanford.edu/~amaas/data/sentiment/. "
      ],
      "metadata": {
        "id": "aYD73eRqblOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to create a tokenizer on sentiment score based on the words used in the movie reviews, so we can analysis whether the reviews are positive or negative."
      ],
      "metadata": {
        "id": "mgm4Ndsjaaod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, json \n",
        "imdb_corpus = []\n",
        "with urllib.request.urlopen(\"https://storage.googleapis.com/wd13/IMDBReviewSent.txt\") as url:\n",
        "  for line in url.readlines():\n",
        "    imdb_corpus.append(line.decode().split('\\t'))"
      ],
      "metadata": {
        "id": "JVwy5gnFYSVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the text and label of document 16\n",
        "docid = 16\n",
        "print(imdb_corpus[docid])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xppAWdKJgRD3",
        "outputId": "f5016ed2-7a9b-4e03-a7a8-fcc65059ffe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive', \"Some films just simply should not be remade. This is one of them. In and of itself it is not a bad film. But it fails to capture the flavor and the terror of the 1963 film of the same title. Liam Neeson was excellent as he always is, and most of the cast holds up, with the exception of Owen Wilson, who just did not bring the right feel to the character of Luke. But the major fault with this version is that it strayed too far from the Shirley Jackson story in it's attempts to be grandiose and lost some of the thrill of the earlier film in a trade off for snazzier special effects. Again I will say that in and of itself it is not a bad film. But you will enjoy the friction of terror in the older version much more.\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the label of document 16\n",
        "docid = 16\n",
        "print(imdb_corpus[docid][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujCCOXJFmp6o",
        "outputId": "65819899-8863-46f1-85f6-dea82b70ae25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the text of document 16\n",
        "docid = 16\n",
        "print(imdb_corpus[docid][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heI9vtSzmqHT",
        "outputId": "1b619966-81b8-4aef-da06-17e2bafb3da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some films just simply should not be remade. This is one of them. In and of itself it is not a bad film. But it fails to capture the flavor and the terror of the 1963 film of the same title. Liam Neeson was excellent as he always is, and most of the cast holds up, with the exception of Owen Wilson, who just did not bring the right feel to the character of Luke. But the major fault with this version is that it strayed too far from the Shirley Jackson story in it's attempts to be grandiose and lost some of the thrill of the earlier film in a trade off for snazzier special effects. Again I will say that in and of itself it is not a bad film. But you will enjoy the friction of terror in the older version much more.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a tokenizer"
      ],
      "metadata": {
        "id": "12DyunxPvThg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def tokenize(doc):\n",
        "  emoti_list = [':)','(:',':(','):',':D','D:',':P','P:',':V','V:',':/','/:',':\\\\','\\\\:',':|','|:',\n",
        "                ';)','(;',';(',');',';D','D;',';P','P;',';V','V;',';/','/;',';\\\\','\\\\;',';|','|;',\n",
        "                ':-)','(-:',':-(',')-:',':-D','D-:',':-P','P-:',':-V','V-:',':-/','/-:',':-\\\\',\n",
        "                '\\\\-:',':-|','|-:',';-)','(-;',';-(',')-;',';-D','D-;',';-P','P-;',';-V','V-;',\n",
        "                ';-/','/-;',';-\\\\','\\\\-;',';-|','|-;']\n",
        "  tokenizer_pattern = re.compile('|'.join([\n",
        "      '|'.join([re.escape(e) for e in emoti_list]),\n",
        "      \"[A-Za-z]+(?:['-_\\.][A-Za-z]+)?\",\n",
        "      '\\.\\.+'\n",
        "      ]))\n",
        "  tokens = tokenizer_pattern.findall(doc)\n",
        "  for i in range(0,len(tokens)):\n",
        "    if re.match('\\.\\.+',tokens[i]):\n",
        "      tokens[i] = '..+'\n",
        "    else:\n",
        "      tokens[i] = tokens[i].lower()\n",
        "  return(tokens)"
      ],
      "metadata": {
        "id": "KCag6LMXvThr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import log function\n",
        "from math import log\n",
        "log(1)"
      ],
      "metadata": {
        "id": "BoHVDK-qHC1Z",
        "outputId": "b0c7a818-fee7-4c29-cbf1-0bfbc3550fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a lexicon"
      ],
      "metadata": {
        "id": "dxttX-SIh1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate sentiment scores for every token in the corpus\n",
        "positive_count = 0\n",
        "negative_count = 0\n",
        "\n",
        "token_positive_count = {}\n",
        "token_negative_count = {}\n",
        "\n",
        "unique_tokens = set()\n",
        "\n",
        "for doc in imdb_corpus:\n",
        "\n",
        "  label = doc[0]\n",
        "  tokens = tokenize(doc[1])\n",
        "\n",
        "  if label=='positive':\n",
        "    positive_count += 1\n",
        "  else:\n",
        "    negative_count += 1\n",
        "\n",
        "  for token in set(tokens):\n",
        "    unique_tokens.add(token)\n",
        "    if label=='positive':\n",
        "      if token not in token_positive_count:\n",
        "        token_positive_count[token] = 0\n",
        "      token_positive_count[token] += 1\n",
        "    else:\n",
        "      if token not in token_negative_count:\n",
        "        token_negative_count[token] = 0\n",
        "      token_negative_count[token] += 1\n",
        "\n",
        "\n",
        "lexicon = {}\n",
        "\n",
        "for token in lexicon:\n",
        "  if token not in token_positive_count or token not in token_negative_count:\n",
        "    continue\n",
        "  lexicon[token] = log((token_positive_count[token]/positive_count)/(token_negative_count[token]/negative_count))"
      ],
      "metadata": {
        "id": "8zPLVa33h3IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Create a score message function\n",
        " def score_message(doc):\n",
        "  score = 0\n",
        "  for token in set(tokenize(doc)):\n",
        "    if token in lexicon:\n",
        "      score += lexicon[token]\n",
        "  return(score)"
      ],
      "metadata": {
        "id": "ss48vDhckZMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Live Twitter Data Dataset"
      ],
      "metadata": {
        "id": "HoQ1o-ixdCyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> I have downloaded the live tweets data by Twitter API bearer token through Twitter official. I am going to extract the recent 1000 tweets related to Netflix & Disney plus, create a tokenizer to calculate the sentiment score of the tweets, in order to generate the summary statistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "EVNSoVNZbQIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded_files = files.upload()\n",
        "twitter_bearer_token = uploaded_files['twitter_bearer_token.txt'].decode()"
      ],
      "metadata": {
        "id": "4QGiT3JzEkfC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "5ffa2e33-0723-412c-b91f-e5e7063b79c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13cf20e5-85b4-47f5-bbd7-c6a064ee3f99\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13cf20e5-85b4-47f5-bbd7-c6a064ee3f99\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving twitter_bearer_token.txt to twitter_bearer_token.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query your search query\n",
        "#bearer_token your Twitter API bearer token\n",
        "#number_of_tweets the number of tweets you want to return\n",
        "import requests\n",
        "def get_tweets(query,bearer_token,number_of_tweets):\n",
        "  tweets = []\n",
        "  next_token = None\n",
        "  while len(tweets)<number_of_tweets:\n",
        "    response = requests.get(\n",
        "        url = 'https://api.twitter.com/2/tweets/search/recent',\n",
        "        params = {\n",
        "          'query':query,\n",
        "          'next_token':next_token},\n",
        "        headers = {'authorization' : 'bearer '+bearer_token} \n",
        "        )\n",
        "    response_json = response.json()\n",
        "    for tweet in response_json['data']:\n",
        "      tweets.append(tweet)\n",
        "    if 'next_token' not in response_json['meta']:\n",
        "      break\n",
        "    next_token = response_json['meta']['next_token']\n",
        "  return(tweets)"
      ],
      "metadata": {
        "id": "6kBRzk3HGAyK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = get_tweets('NETFLIX OR DISNEY+',twitter_bearer_token,100)"
      ],
      "metadata": {
        "id": "BuOeGOd-GONs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "e25b2c9b-0c17-473f-b332-4c61cd398c19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9bf3f48c5a07>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NETFLIX OR DISNEY+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtwitter_bearer_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b92be5397fdc>\u001b[0m in \u001b[0;36mget_tweets\u001b[0;34m(query, bearer_token, number_of_tweets)\u001b[0m\n\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'next_token'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "Pnvxo4rcGQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('tweets.json','w') as f:\n",
        "  f.write(json.dumps(tweets))\n",
        "files.download('tweets.json')"
      ],
      "metadata": {
        "id": "4d8rF0aWGSeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "uploaded_files = files.upload()\n",
        "tweets = json.loads(uploaded_files['tweets.json'])"
      ],
      "metadata": {
        "id": "61Vh3EyTdDXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[0]"
      ],
      "metadata": {
        "id": "42IVmcDEduMj",
        "outputId": "97061262-85b4-4b7c-8465-1c2161b7521b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'edit_history_tweet_ids': ['1623404827190476801'],\n",
              " 'id': '1623404827190476801',\n",
              " 'text': \"RT @ToughPigs: Today on ToughPigs, we're sharing the incredible Muppet fan art of @KOMakesThings! Featuring Miss Piggy as a bunch of Disney…\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze Twitter Data"
      ],
      "metadata": {
        "id": "ietSm--hqKSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install lexicon\n",
        "\n",
        "from google.colab import files\n",
        "uploaded_files = files.upload()\n",
        "lexicon_file = uploaded_files['lexicon.txt'].decode()\n",
        "lexicon = {}\n",
        "for line in lexicon_file.split('\\n'): \n",
        "  split_line = line.split('\\t')\n",
        "  token = split_line[0]\n",
        "  score = float(split_line[1])\n",
        "  lexicon[token] = score \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UuVvWbtNJmLO",
        "outputId": "a4c4bffa-cbae-4992-cb44-1982ec21d1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-711e930e-7f2e-4a35-8b08-0a9fbfca2df3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-711e930e-7f2e-4a35-8b08-0a9fbfca2df3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lexicon.txt to lexicon.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment score \n",
        "def sentiment_score(tweets):\n",
        "  score = 0\n",
        "  for token in set(tokenize(tweets)):\n",
        "    if token in lexicon:\n",
        "      score += lexicon[token]\n",
        "  return(score)"
      ],
      "metadata": {
        "id": "s91-R7utNhw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet_live_text = [tweet['text'] for tweet in tweets]"
      ],
      "metadata": {
        "id": "0xyCLMxyzcuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet_live_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kMvsBYhwfho4",
        "outputId": "0070e463-aaab-4427-9bec-68951f37bd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RT @ToughPigs: Today on ToughPigs, we're sharing the incredible Muppet fan art of @KOMakesThings! Featuring Miss Piggy as a bunch of Disney…\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_total_count = 0\n",
        "netflix_positive_count = 0\n",
        "netflix_negative_count = 0\n",
        "\n",
        "disneyplus_total_count = 0\n",
        "disneyplus_positive_count = 0\n",
        "disneyplus_negative_count = 0\n",
        "for tweet in tweets:\n",
        "  tweet_sentiment_score = sentiment_score(tweet['text'])\n",
        "  tweet_tokens = set(tokenize(tweet['text']))\n",
        "  if {'netflix'}.intersection(tweet_tokens):\n",
        "    netflix_total_count += 1\n",
        "    if tweet_sentiment_score>=0:\n",
        "      netflix_positive_count += 1\n",
        "    else:\n",
        "      netflix_negative_count += 1\n",
        "  # print(tweet_tokens)\n",
        "  if {'disney'}.intersection(tweet_tokens):\n",
        "    disneyplus_total_count += 1\n",
        "    if tweet_sentiment_score>=0:\n",
        "      disneyplus_positive_count += 1\n",
        "    else:\n",
        "      disneyplus_negative_count += 1"
      ],
      "metadata": {
        "id": "CMJXQr98OQJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the summary statistics\n",
        "Netflix_summary = {\n",
        "    'NTweets' : netflix_total_count,\n",
        "    'ShareofVoice' : 100*netflix_total_count/(netflix_total_count+disneyplus_total_count),\n",
        "    'PositivePct' : 100*netflix_positive_count/netflix_total_count,\n",
        "    'NegativePct' : 100*netflix_negative_count/netflix_total_count,\n",
        "    'NetPositivePct' : 100*(netflix_positive_count-netflix_negative_count)/netflix_total_count\n",
        "}\n",
        "Disneyplus_summary = {\n",
        "    'NTweets' : disneyplus_total_count,\n",
        "    'ShareofVoice' : 100*disneyplus_total_count/(netflix_total_count+disneyplus_total_count),\n",
        "    'PositivePct' : 100*disneyplus_positive_count/disneyplus_total_count,\n",
        "    'NegativePct' : 100*disneyplus_negative_count/disneyplus_total_count,\n",
        "    'NetPositivePct' : 100*(disneyplus_positive_count-disneyplus_negative_count)/disneyplus_total_count\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(' | '+\n",
        "      'Channels          | '+\n",
        "      '# Tweets       | '+\n",
        "      'Share of Voice | '+\n",
        "      'Positive %     | '+\n",
        "      'Negative %     | '+\n",
        "      'Net Positive % | ')\n",
        "print(\"\")\n",
        "print((\" | \"+\n",
        "      \"Netflix         | \"+\n",
        "      \"{NTweets:5.0f}          | \"+\n",
        "      \"{ShareofVoice:5.2f}%         | \"+\n",
        "      \"{PositivePct:2.2f}%         | \"+\n",
        "      \"{NegativePct:2.2f}%         | \"+\n",
        "      \"{NetPositivePct:2.2f}%         | \").format(**Netflix_summary))\n",
        "print((\" | \"+\n",
        "      \"Disneyplus  | \"+\n",
        "      \"{NTweets:5.0f}          | \"+\n",
        "      \"{ShareofVoice:5.2f}%         | \"+\n",
        "      \"{PositivePct:2.2f}%         | \"+\n",
        "      \"{NegativePct:2.2f}%         | \"+\n",
        "      \"{NetPositivePct:2.2f}%         | \").format(**Disneyplus_summary))\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "AzDdW77wbUl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d134dbae-026b-4abd-92d5-8096d6d8f137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " | Channels          | # Tweets       | Share of Voice | Positive %     | Negative %     | Net Positive % | \n",
            "\n",
            " | Netflix         |    63          | 73.26%         | 52.38%         | 47.62%         | 4.76%         | \n",
            " | Disneyplus  |    23          | 26.74%         | 56.52%         | 43.48%         | 13.04%         | \n",
            "\n"
          ]
        }
      ]
    }
  ]
}